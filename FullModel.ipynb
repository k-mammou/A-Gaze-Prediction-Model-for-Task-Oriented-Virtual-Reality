{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25269,
     "status": "ok",
     "timestamp": 1730814774672,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "DRQ4mBSyvSgS",
    "outputId": "47b4bb37-c052-47d1-c4b0-eef8fb8b71ef"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5024,
     "status": "ok",
     "timestamp": 1730814781949,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "3OIjfOOQghjq",
    "outputId": "0407545a-363b-47f0-de7c-3f244bc617ad"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "#Function to separate indices of dataset to training, validation & testing\n",
    "#Returns: final index for each\n",
    "#also num of total samples\n",
    "def split_train_val_test(filenames):\n",
    "    # Calculate cumulative sizes for each file\n",
    "    file_sizes = []\n",
    "    total_samples = 0\n",
    "\n",
    "    # Calculate size of each file and add to get total number of samples\n",
    "    for filename in filenames:\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            file_size = len(f['X1_dataset'])\n",
    "            file_sizes.append(file_size)\n",
    "            total_samples += file_size\n",
    "            print(f\"Size: {file_size} for file:{filename}\")\n",
    "    print(\"Total samples: \", total_samples)\n",
    "    train = total_samples * 0.8\n",
    "    val = train + 0.1 * total_samples\n",
    "    test = total_samples\n",
    "    return int(train), int(val), int(test), total_samples\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "def datasetGenerator(filenames, batchsize, startPoint, endPoint, limit):\n",
    "    # Init list of file sizes\n",
    "    file_sizes = []\n",
    "    total_samples = 0\n",
    "\n",
    "    # Calculate size of each file\n",
    "    for filename in filenames:\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            file_size = len(f['X1_dataset'])\n",
    "            file_sizes.append(file_size)\n",
    "            total_samples += file_size\n",
    "\n",
    "    # Adjust the endPoint based on limit, if provided\n",
    "    if limit is None:\n",
    "        limit = endPoint\n",
    "    if limit < endPoint:\n",
    "        endPoint = limit\n",
    "\n",
    "    # Start from startPoint\n",
    "    current_position = startPoint\n",
    "\n",
    "    while True:\n",
    "        # Find the current file and local position based on `current_position`\n",
    "        file_index = 0\n",
    "        cumulative_position = 0\n",
    "        added_file_sizes_position = 0\n",
    "\n",
    "        # Determine the correct file index and local position                                                         #helps locate the proper file to use (for validation for example)\n",
    "        while file_index < len(file_sizes) and current_position >= added_file_sizes_position + file_sizes[file_index]:\n",
    "            added_file_sizes_position += file_sizes[file_index]\n",
    "            file_index += 1\n",
    "\n",
    "        if file_index == len(file_sizes):\n",
    "            # If we've reached beyond the last file, reset\n",
    "            current_position = startPoint\n",
    "            continue\n",
    "\n",
    "        # Local position within the current file\n",
    "        local_position = current_position - added_file_sizes_position\n",
    "        filename = filenames[file_index]\n",
    "\n",
    "        # Print the current filename being used\n",
    "        #print(f\"Using file with index {file_index}: {filename}\")\n",
    "\n",
    "\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            X1_dataset = f['X1_dataset']\n",
    "            X2_dataset = f['X2_dataset']\n",
    "            Y_dataset = f['Y_dataset']\n",
    "\n",
    "            # Calculate the number of samples in the current file\n",
    "            num_samples = file_sizes[file_index]\n",
    "\n",
    "            # Continue from the current position within this file\n",
    "            while local_position < num_samples and current_position < endPoint:\n",
    "                # Calculate batch indices\n",
    "                batch_end = min(local_position + batchsize, num_samples)\n",
    "                batch_indices = np.arange(local_position, batch_end)\n",
    "\n",
    "                # Create batches\n",
    "                X1batch = X1_dataset[batch_indices]\n",
    "                X2batch = X2_dataset[batch_indices]\n",
    "                YBatch = Y_dataset[batch_indices]\n",
    "\n",
    "                # Yield the current batch\n",
    "                yield (X1batch, X2batch), YBatch\n",
    "\n",
    "                # Update positions\n",
    "                local_position += batchsize\n",
    "                current_position += batchsize\n",
    "\n",
    "            # Move to the next file\n",
    "            local_position = 0\n",
    "\n",
    "        # If we've exhausted all files, reset the position to startPoint\n",
    "        if current_position >= endPoint:\n",
    "            current_position = startPoint\n",
    "\n",
    "#######################################################################################################\n",
    "final_path1 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dataset.hdf5\"\n",
    "final_path2 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset2.hdf5\"\n",
    "final_path3 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset3.hdf5\"\n",
    "final_path4 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset4.hdf5\"\n",
    "\n",
    "paths = [final_path1, final_path2,final_path3,final_path4]\n",
    "\n",
    "#Parameters settings\n",
    "batchsize = 64\n",
    "limit = None\n",
    "train, val, test, tot_samples = split_train_val_test(paths)\n",
    "\n",
    "print(\"\\nTraining index:\", train, \"\\nValidation index:\", val, \"\\nTesting index:\", test)\n",
    "\n",
    "\n",
    "\n",
    "#create generators\n",
    "TrainingDatasetGenerator = datasetGenerator(paths, batchsize, 0, train, limit)\n",
    "ValidationDatasetGenerator = datasetGenerator(paths, batchsize, train,val, limit)\n",
    "TestingDatasetGenerator = datasetGenerator(paths, batchsize, val, test, limit)\n",
    "\n",
    "\n",
    "tcount=0\n",
    "#for X2batch, YBatch in TrainingDatasetGenerator:\n",
    "#   tcount+=1\n",
    "#   print(tcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3454,
     "status": "ok",
     "timestamp": 1730814788522,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "F20vCfLpO8ia"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling3D\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2700,
     "status": "ok",
     "timestamp": 1730814794761,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "Oymn7qbHPt8q",
    "outputId": "9ac2fa74-de50-43b9-fde0-c8096d71c4ba"
   },
   "outputs": [],
   "source": [
    "#first submodel\n",
    "input1 = Input(shape=(10,71,128,3))\n",
    "convlstm1 = ConvLSTM2D(filters = 16,kernel_size=(3,3),padding = \"same\", return_sequences= True, activation=\"relu\")(input1)\n",
    "pooling1 = MaxPooling3D(pool_size=(1, 3, 3), padding='same')(convlstm1)\n",
    "\n",
    "convlstm2 = ConvLSTM2D(filters = 16,kernel_size=(3,3),padding = \"same\", return_sequences= True, activation=\"relu\")(pooling1)\n",
    "pooling2 = MaxPooling3D(pool_size=(1, 4, 4), padding='same')(convlstm2)\n",
    "\n",
    "\n",
    "convlstm3 = ConvLSTM2D(filters = 16,kernel_size=(3,3),padding = \"same\", return_sequences= True, activation=\"relu\")(pooling2)\n",
    "pooling3 = MaxPooling3D(pool_size=(1, 1, 2), padding='same')(convlstm3)\n",
    "\n",
    "\n",
    "convlstm4 = ConvLSTM2D(filters = 16,kernel_size=(3,3),padding = \"same\", return_sequences= True, activation=\"relu\")(pooling3)\n",
    "pooling4 = MaxPooling3D(pool_size=(1, 2, 2), padding='same')(convlstm4)\n",
    "\n",
    "convlstm5 = ConvLSTM2D(filters = 16,kernel_size=(3,3),padding = \"same\", return_sequences= False, activation=\"relu\")(pooling4)\n",
    "\n",
    "flat1 = Flatten()(convlstm5)\n",
    "dense11 = Dense(64,activation='relu')(flat1)\n",
    "\n",
    "drop11 = Dropout(0.5)(dense11)\n",
    "\n",
    "dense12 = Dense(32, activation = 'relu') (drop11)\n",
    "dense13 = Dense(32, activation = 'relu') (dense12)\n",
    "dense14 = Dense(16, activation = 'relu') (dense13)\n",
    "\n",
    "\n",
    "#second submodel\n",
    "input2 = Input(shape=(10,2))\n",
    "lstm1 = LSTM(units=20,activation = 'relu', return_sequences = True)(input2)\n",
    "lstm2 = LSTM(units=20,activation = 'relu', return_sequences = True)(lstm1)\n",
    "lstm3 = LSTM(units=10,activation = 'relu', return_sequences = True)(lstm2)\n",
    "lstm4 = LSTM(units=10,activation = 'relu', return_sequences = False)(lstm3)\n",
    "\n",
    "flat21 = Flatten()(lstm4)\n",
    "dense21 = Dense(32, activation='relu')(flat21)\n",
    "\n",
    "drop21 = Dropout(0.5)(dense21)\n",
    "dense22 = Dense(16, activation = 'relu') (drop21)\n",
    "\n",
    "\n",
    "####FUSION\n",
    "merge = keras.layers.Maximum()([dense14,dense22])\n",
    "\n",
    "\n",
    "hidden1 = Dense(16, activation = 'relu') (merge)\n",
    "drop1 = Dropout(0.5) (hidden1)\n",
    "\n",
    "output = Dense(2, activation = 'sigmoid')(drop1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create model\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "#compile\n",
    "#compile\n",
    "model.compile(\n",
    "    loss=keras.losses.MAE,\n",
    "    optimizer= keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[keras.metrics.MSE, keras.metrics.CosineSimilarity()]\n",
    ")\n",
    "#summary\n",
    "#plot_model(model)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22134450,
     "status": "ok",
     "timestamp": 1730836948730,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "jmwQbOq1bNrj",
    "outputId": "14e7cc0b-479f-437c-886a-411587d72934"
   },
   "outputs": [],
   "source": [
    "#define some callbacks to improve training\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 10, verbose = 1)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", patience = 5, verbose = 1)\n",
    "\n",
    "\n",
    "checkpoint_path = \"/content/drive/MyDrive/Thesis/Models/training_full10.weights.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                verbose=0)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "#start timer to see how long the training takes\n",
    "start = time()\n",
    "\n",
    "#Train\n",
    "training_history = model.fit(\n",
    "    TrainingDatasetGenerator,\n",
    "    epochs= epochs,\n",
    "    steps_per_epoch=(train // batch_size),\n",
    "    validation_data=ValidationDatasetGenerator,\n",
    "    validation_steps = ((val-train)// batch_size),\n",
    "    callbacks= [early_stopping, reduce_lr, cp_callback],\n",
    "    verbose=1\n",
    ")\n",
    "finish = (time() - start) / 60\n",
    "print(\"Training time: \", finish, \" minutes\")\n",
    "model.save(\"/content/drive/MyDrive/Thesis/Models/Full_10.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "executionInfo": {
     "elapsed": 1528,
     "status": "ok",
     "timestamp": 1730836952664,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "ofBBKLqeYIVl",
    "outputId": "2fbc46e6-bf76-4aea-9501-dc1b3a1f5146"
   },
   "outputs": [],
   "source": [
    "#model.save(\"/content/drive/MyDrive/Thesis/Models/Full_5.h5\")\n",
    "#print(\"Saved model to disk\")\n",
    "\n",
    "#model evaluation!!\n",
    "\n",
    "#LOSS FUNCTION\n",
    "f1 = plt.figure(\"Loss\")\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(training_history.history['loss'], label='training set')\n",
    "plt.plot(training_history.history['val_loss'], label='validation set')\n",
    "plt.suptitle('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1730836952665,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "xjr_rBt4Y5OT",
    "outputId": "9d22d1dd-c8ef-4f6a-b0db-2ead737c4eb0"
   },
   "outputs": [],
   "source": [
    "f2 = plt.figure(\"MSE Metric\")\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('MSE')\n",
    "plt.plot(training_history.history['mean_squared_error'], label='training set')\n",
    "plt.plot(training_history.history['val_mean_squared_error'], label='validation set')\n",
    "plt.suptitle('MSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1730836952666,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "GkSTlktTZDWy",
    "outputId": "0b8df0f8-f4b4-4868-90eb-11efd06f7155"
   },
   "outputs": [],
   "source": [
    "f3 = plt.figure(\"Cosine Similarity\")\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('cosine similarity')\n",
    "plt.plot(training_history.history['cosine_similarity'], label='training set')\n",
    "plt.plot(training_history.history['val_cosine_similarity'], label='validation set')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.suptitle('Cosine Similarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363265,
     "status": "ok",
     "timestamp": 1730837352308,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "F3A6cNmEaabY",
    "outputId": "61e41292-f4dc-4f3c-c4ce-aea62568ce58"
   },
   "outputs": [],
   "source": [
    "TestingDatasetGenerator = datasetGenerator(paths, batchsize, val, test, limit)\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(TestingDatasetGenerator,steps =((test-val)// batch_size))\n",
    "print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32575,
     "status": "ok",
     "timestamp": 1726841633353,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -180
    },
    "id": "HRRMS3wzx0uD",
    "outputId": "8d33cfbf-8210-4124-f31c-731b7c6629be"
   },
   "outputs": [],
   "source": [
    "#Test predictions using unseen data!\n",
    "\n",
    "test_predictions = model.predict(TestingDatasetGenerator, steps = (int(test-val)// batch_size))\n",
    "print(\"predictions shape:\", test_predictions.shape)\n",
    "print(\"Predictions\")\n",
    "print(test_predictions)\n",
    "#save data to files\n",
    "np.save('/content/drive/MyDrive/Thesis/Datasets2/FULL_model_preds.npy',test_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKc674a/YLuON02oi1FEby",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
