{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36348,
     "status": "ok",
     "timestamp": 1732281009312,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "RDfpxHcJ_Flu",
    "outputId": "c4e956de-b4a4-4e26-d701-d3d430ae8c40"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732281009313,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "qqnZ8psB_NAx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#calculate angular error between two points\n",
    "def angular_difference(theta1, phi1, theta2, phi2):\n",
    "    delta_theta = theta1 - theta2\n",
    "    delta_phi = phi1 - phi2\n",
    "    return delta_theta, delta_phi\n",
    "\n",
    "#Calculate final angular error for all points\n",
    "def AngularError(data1, data2):\n",
    "  delta_thetas, delta_phis = angular_difference(data1[:,0], data1[:,1], data2[:,0], data2[:,1])\n",
    "  full_errors = np.sqrt(delta_thetas**2 + delta_phis**2)                                          #Overall angular error (Euclidean distance in angular space)\n",
    "  #mean angular\n",
    "  mean_error = np.mean(full_errors)\n",
    "  return full_errors, mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732281009313,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "sXkn_M63_brd"
   },
   "outputs": [],
   "source": [
    "def denormalize(data,true_min, true_max):\n",
    "  denormalized = data * (true_max - true_min) + true_min\n",
    "  return denormalized\n",
    "\n",
    "\n",
    "def denormalize_full(data, true_minX, true_maxX, true_minY, true_maxY):  #data = (x,y)\n",
    "  Xgazedenorm = denormalize(data[:,0], true_minX, true_maxX)   #denorm\n",
    "  Ygazedenorm = denormalize(data[:,1], true_minY, true_maxY)\n",
    "\n",
    "  Xgazedenorm = np.array(Xgazedenorm)\n",
    "  Ygazedenorm = np.array(Ygazedenorm)\n",
    "  final_denormalized = np.column_stack((Xgazedenorm,Ygazedenorm))\n",
    "\n",
    "\n",
    "  return final_denormalized     #data shape (x,y)\n",
    "\n",
    "\n",
    "Xmin = -19.067780871643322\n",
    "Xmax = 21.46873726490827\n",
    "\n",
    "Ymin = -38.03606803389029\n",
    "Ymax = 27.631278570342523\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3087,
     "status": "ok",
     "timestamp": 1732281012395,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "5GTsASUVAqOF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732281012395,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "1NQtMKSIOUBe"
   },
   "outputs": [],
   "source": [
    "#Remove outliers from one-dimensional array\n",
    "#input: 1d array\n",
    "#output: 1d array without outliers\n",
    "def removeOutliers1D(data):\n",
    "  #print(\"Old Shape:\", data.shape)\n",
    "  # IQR\n",
    "  Q1 = np.percentile(data, 25, method='midpoint')\n",
    "  Q3 = np.percentile(data, 75, method='midpoint')\n",
    "  IQR = Q3 - Q1\n",
    "  #print(\"IQR: \",IQR)\n",
    "  # Calculate the upper and lower limits\n",
    "  upper = Q3 + 1.5 * IQR\n",
    "  lower = Q1 - 1.5 * IQR\n",
    "  #Find Outlier values below and above thresholds (indexes)\n",
    "  upper_array = np.where(data >= upper)\n",
    "  lower_array = np.where(data <= lower)\n",
    "\n",
    "  #Concatenate outliers\n",
    "  outliers = np.hstack((upper_array,lower_array))\n",
    "  print(\"Outliers:\" , outliers.shape[1])\n",
    "  #print(outliers)\n",
    "\n",
    "  #Remove outliers\n",
    "  #data = np.delete(data, outliers)\n",
    "  #print(\"New shape:\", data.shape)\n",
    "\n",
    "  return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 112700,
     "status": "ok",
     "timestamp": 1732282696592,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "GRnEMQqWA1y9",
    "outputId": "f7ce52d6-554e-48a0-d6c0-9b1eff529b8d"
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "model = models.load_model(\"/content/drive/MyDrive/Thesis/Models/Full_10.h5\")\n",
    "\n",
    "final_path1 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dataset.hdf5\"\n",
    "final_path2 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset2.hdf5\"\n",
    "final_path3 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset3.hdf5\"\n",
    "final_path4 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset4.hdf5\"\n",
    "\n",
    "filenames = [final_path1, final_path2, final_path3, final_path4]\n",
    "\n",
    "\n",
    "#MEAN baseline                              #calc mean for X and for Y from groundturh data, and then put it in an array (X,Y) for proper shape (curr mean: 1.1891, -5.2595)\n",
    "mean_theta = 1.1891425755626226\n",
    "mean_phi = -5.259580611647258\n",
    "mean_baseline = np.full((1,2), (mean_theta, mean_phi))\n",
    "print(f\"Mean Baseline: {mean_baseline}\")\n",
    "\n",
    "#CENTER baseline                            # (0,0)\n",
    "center_theta = center_phi = 0\n",
    "center_baseline = np.full((1,2),0)\n",
    "print(f\"Center baseline: {center_baseline}\")\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "startPoint = 154863\n",
    "endPoint = 172071\n",
    "\n",
    "#init prediction list\n",
    "full_predictions = []\n",
    "full_groundtruth = []\n",
    "\n",
    "#init full error list\n",
    "full_errors= []\n",
    "full_error_mean = []\n",
    "full_error_center = []\n",
    "\n",
    "# Init list of file sizes\n",
    "file_sizes = []\n",
    "total_samples = 0\n",
    "\n",
    "# Calculate size of each file\n",
    "for filename in filenames:\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        file_size = len(f['X1_dataset'])\n",
    "        file_sizes.append(file_size)\n",
    "        total_samples += file_size\n",
    "\n",
    "\n",
    "# Start from startPoint\n",
    "current_position = startPoint\n",
    "\n",
    "# Find the current file and local position based on `current_position`\n",
    "file_index = 0\n",
    "cumulative_position = 0\n",
    "added_file_sizes_position = 0\n",
    "\n",
    " # Determine the correct file index and local position                                                         #helps locate the proper file to use (for validation for example)\n",
    "while file_index < len(file_sizes) and current_position >= added_file_sizes_position + file_sizes[file_index]:\n",
    "     added_file_sizes_position += file_sizes[file_index]\n",
    "     file_index += 1\n",
    "\n",
    " # Local position within the current file\n",
    "local_position = current_position - added_file_sizes_position\n",
    "filename = filenames[file_index]\n",
    "\n",
    "# Print the current filename being used\n",
    "print(f\"\\nUsing file with index {file_index}: {filename}\")\n",
    "\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    X1_dataset = f['X1_dataset']\n",
    "    X2_dataset = f['X2_dataset']\n",
    "    Y_dataset = f['Y_dataset']\n",
    "    num_samples = file_sizes[file_index]\n",
    "\n",
    "    while local_position < num_samples and current_position < endPoint:\n",
    "        # Calculate batch indices\n",
    "        batch_end = min(local_position + batch_size, num_samples)\n",
    "        batch_indices = np.arange(local_position, batch_end)\n",
    "        #print(batch_indices)\n",
    "        #create batches!\n",
    "        X1batch = X1_dataset[batch_indices]\n",
    "        X2batch = X2_dataset[batch_indices]\n",
    "        Ybatch  = Y_dataset[batch_indices]\n",
    "        batch_predictions = model.predict((X1batch,X2batch))        #predict\n",
    "\n",
    "        final_batch_predictions = denormalize_full(batch_predictions, Xmin, Xmax, Ymin, Ymax)\n",
    "        final_batch_groundTruth = denormalize_full(Ybatch, Xmin, Xmax, Ymin, Ymax)\n",
    "        full_predictions.append(final_batch_predictions)\n",
    "        full_groundtruth.append(final_batch_groundTruth)\n",
    "\n",
    "\n",
    "        batch_errors, batch_average = AngularError(final_batch_groundTruth, final_batch_predictions)\n",
    "        full_errors.append(batch_errors)\n",
    "\n",
    "        mean_batch_error, mean_batch_average = AngularError(final_batch_groundTruth,mean_baseline)\n",
    "        full_error_mean.append(mean_batch_error)\n",
    "\n",
    "        center_batch_error, mean_batch_average = AngularError(final_batch_groundTruth,center_baseline)\n",
    "        full_error_center.append(center_batch_error)\n",
    "\n",
    "\n",
    "\n",
    "        # Update positions\n",
    "        local_position += batch_size\n",
    "        current_position += batch_size\n",
    "\n",
    "\n",
    "final_prediction_error = []\n",
    "final_mean_error = []\n",
    "final_center_error = []\n",
    "\n",
    "final_predictions = []\n",
    "final_groundtruth = []\n",
    "\n",
    "for batch in full_predictions:\n",
    "  for pred in batch:\n",
    "    final_predictions.append(pred)\n",
    "\n",
    "\n",
    "for batch in full_groundtruth:\n",
    "  for pred in batch:\n",
    "    final_groundtruth.append(pred)\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "for batch in full_errors:\n",
    "  for pred in batch:\n",
    "    final_prediction_error.append(pred)\n",
    "print(\"Final num of predictions: \",len(final_prediction_error))\n",
    "\n",
    "#mean_final = np.mean(final_prediction_error)\n",
    "#print(\"Average: \", mean_final)\n",
    "\n",
    "for batch in full_error_mean:\n",
    "  for pred in batch:\n",
    "    final_mean_error.append(pred)\n",
    "\n",
    "\n",
    "for batch in full_error_center:\n",
    "  for pred in batch:\n",
    "    final_center_error.append(pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1732281754014,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "aQTESHSGNLo_",
    "outputId": "228b6217-6de2-4bf3-979a-05b451ab2889"
   },
   "outputs": [],
   "source": [
    "data = [final_center_error, final_mean_error, final_prediction_error]\n",
    "print(\"Mean Baseline   - Average Error: \", np.mean(final_mean_error),\n",
    "      \"\\nCenter Baseline - Average Error: \",np.mean(final_center_error),\n",
    "      \"\\nPredictions     - Average Error:  \", np.mean(final_prediction_error), \"\\n\")\n",
    "print(\"Mean Baseline   - Median Value: \", np.median(final_mean_error),\n",
    "      \"\\nCenter Baseline - Median Value: \", np.median(final_center_error),\n",
    "      \"\\nPredictions     - Median Value:  \", np.median(final_prediction_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "executionInfo": {
     "elapsed": 2191,
     "status": "ok",
     "timestamp": 1731586654094,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "Br8MVUGRNSVT",
    "outputId": "a3e54fbf-bc00-4320-e0d2-0b914309899b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#fig = plt.figure(figsize =(10, 7))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.boxplot(data=data, width=0.5,showmeans=True, palette ='flare')\n",
    "ax.set_xticklabels(['Center Baseline', 'Mean Baseline', 'Predictions'])\n",
    "# Add titles to the axes\n",
    "#ax.set_xlabel('Predictors')  # X-axis label\n",
    "ax.set_ylabel('Angular Error (degrees)')  # Y-axis label\n",
    "\n",
    "# Add a title to the plot\n",
    "#ax.set_title('Comparison of Errors Across Different Baselines and Predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1731586658414,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "RHzGDGSWOoo2",
    "outputId": "a47113a1-8d6c-4d42-f73e-ab25b79e21b4"
   },
   "outputs": [],
   "source": [
    "print(\"==========================Predictions==============================\")\n",
    "removeOutliers1D(final_prediction_error)\n",
    "print(\"==========================Mean baseline=============================\")\n",
    "removeOutliers1D(final_mean_error)\n",
    "print(\"==========================Center baseline===========================\")\n",
    "removeOutliers1D(final_center_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3539,
     "status": "ok",
     "timestamp": 1731330357938,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "j5GtDmE7itlY",
    "outputId": "2942216a-8e66-4c86-cd9f-9b2e843fd967"
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "model = models.load_model(\"/content/drive/MyDrive/Thesis/Models/Full_10.h5\")\n",
    "\n",
    "final_path1 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dataset.hdf5\"\n",
    "final_path2 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset2.hdf5\"\n",
    "final_path3 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset3.hdf5\"\n",
    "final_path4 = \"/content/drive/MyDrive/Thesis/Datasets2/final_dtset/final_dataset4.hdf5\"\n",
    "\n",
    "filenames = [final_path1, final_path2, final_path3, final_path4]\n",
    "\n",
    "\n",
    "#MEAN baseline                              #calc mean for X and for Y from groundturh data, and then put it in an array (X,Y) for proper shape (curr mean: 1.1891, -5.2595)\n",
    "mean_theta = 1.1891425755626226\n",
    "mean_phi = -5.259580611647258\n",
    "mean_baseline = np.full((1,2), (mean_theta, mean_phi))\n",
    "print(f\"Mean Baseline: {mean_baseline}\")\n",
    "\n",
    "#CENTER baseline                            # (0,0)\n",
    "center_theta = center_phi = 0\n",
    "center_baseline = np.full((1,2),0)\n",
    "print(f\"Center baseline: {center_baseline}\")\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "startPoint = 154863\n",
    "endPoint = 172071\n",
    "\n",
    "\n",
    "\n",
    "#init full error list\n",
    "full_errors= []\n",
    "full_error_mean = []\n",
    "full_error_center = []\n",
    "\n",
    "# Init list of file sizes\n",
    "file_sizes = []\n",
    "total_samples = 0\n",
    "\n",
    "# Calculate size of each file\n",
    "for filename in filenames:\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        file_size = len(f['X1_dataset'])\n",
    "        file_sizes.append(file_size)\n",
    "        total_samples += file_size\n",
    "\n",
    "\n",
    "# Start from startPoint\n",
    "current_position = startPoint\n",
    "\n",
    "# Find the current file and local position based on `current_position`\n",
    "file_index = 0\n",
    "cumulative_position = 0\n",
    "added_file_sizes_position = 0\n",
    "\n",
    "\n",
    " # Determine the correct file index and local position                                                         #helps locate the proper file to use (for validation for example)\n",
    "while file_index < len(file_sizes) and current_position >= added_file_sizes_position + file_sizes[file_index]:\n",
    "     added_file_sizes_position += file_sizes[file_index]\n",
    "     file_index += 1\n",
    "\n",
    " # Local position within the current file\n",
    "local_position = current_position - added_file_sizes_position\n",
    "filename = filenames[file_index]\n",
    "\n",
    "# Print the current filename being used\n",
    "print(f\"\\nUsing file with index {file_index}: {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 214443,
     "status": "ok",
     "timestamp": 1731330577664,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "ts88Nmq8ihuN",
    "outputId": "045b4565-cbb0-4216-cee0-9cd95813300e"
   },
   "outputs": [],
   "source": [
    "# Parameters for region radius\n",
    "ground_truth_radius = 15  # degrees for ground truth region\n",
    "predicted_radius = 15     # degrees for predicted region (both model and baseline)\n",
    "\n",
    "# Initialize lists for recall counting\n",
    "model_hits = 0\n",
    "mean_hits = 0\n",
    "center_hits = 0\n",
    "\n",
    "# Function to calculate Euclidean distance (in degrees) between two sets of points\n",
    "def calculate_distance(gt, pred):\n",
    "    return np.sqrt((pred[:, 0] - gt[:, 0])**2 + (pred[:, 1] - gt[:, 1])**2)\n",
    "\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    X1_dataset = f['X1_dataset']\n",
    "    X2_dataset = f['X2_dataset']\n",
    "    Y_dataset = f['Y_dataset']\n",
    "    num_samples = file_sizes[file_index]\n",
    "\n",
    "    while local_position < num_samples and current_position < endPoint:\n",
    "        # Calculate batch indices\n",
    "        batch_end = min(local_position + batch_size, num_samples)\n",
    "        batch_indices = np.arange(local_position, batch_end)\n",
    "        #print(batch_indices)\n",
    "        #create batches!\n",
    "        X1batch = X1_dataset[batch_indices]\n",
    "        X2batch = X2_dataset[batch_indices]\n",
    "        Ybatch  = Y_dataset[batch_indices]\n",
    "        batch_predictions = model.predict((X1batch,X2batch))        #predict\n",
    "\n",
    "        final_batch_predictions = denormalize_full(batch_predictions, Xmin, Xmax, Ymin, Ymax)\n",
    "        final_batch_groundTruth = denormalize_full(Ybatch, Xmin, Xmax, Ymin, Ymax)\n",
    "\n",
    "        # Calculate distances for model predictions, mean baseline, and center baseline\n",
    "        distances_model = calculate_distance(final_batch_groundTruth, final_batch_predictions)\n",
    "        distances_mean = calculate_distance(final_batch_groundTruth, mean_baseline)\n",
    "        distances_center = calculate_distance(final_batch_groundTruth, center_baseline)\n",
    "\n",
    "        # Count hits based on the ground truth and prediction radii\n",
    "        model_hits += np.sum(distances_model <= ground_truth_radius)\n",
    "        mean_hits += np.sum(distances_mean <= predicted_radius)\n",
    "        center_hits += np.sum(distances_center <= predicted_radius)\n",
    "        # Update positions\n",
    "        local_position += batch_size\n",
    "        current_position += batch_size\n",
    "\n",
    "\n",
    "# Total number of samples evaluated\n",
    "total_samples = endPoint - startPoint\n",
    "\n",
    "# Calculate recall rates\n",
    "model_recall_rate = model_hits / total_samples\n",
    "mean_recall_rate = mean_hits / total_samples\n",
    "center_recall_rate = center_hits / total_samples\n",
    "\n",
    "# Print out the recall rates\n",
    "print(f\"Model Recall Rate: {model_recall_rate:.4f}\")\n",
    "print(f\"Mean Baseline Recall Rate: {mean_recall_rate:.4f}\")\n",
    "print(f\"Center Baseline Recall Rate: {center_recall_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1731331961370,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "wjRa8TDqXc_q",
    "outputId": "7ad766e5-70ff-4a9d-853a-b81bebecfb63"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# No of Data points\n",
    "N = 17208\n",
    "\n",
    "# getting data of the histogram\n",
    "count_mean, bins_count_mean = np.histogram(final_mean_error, bins=10, density = True)\n",
    "count_center, bins_count_center = np.histogram(final_center_error, bins=10, density = True)\n",
    "count_predictions, bins_count_predictions= np.histogram(final_prediction_error, bins=10, density = True)\n",
    "\n",
    "# finding the PDF of the histogram using count values\n",
    "pdf_mean = count_mean / np.sum(bins_count_mean)\n",
    "pdf_center = count_center / np.sum(bins_count_center)\n",
    "pdf_predictions = count_predictions / np.sum(bins_count_predictions)\n",
    "\n",
    "#calculate the CDF\n",
    "cdf_mean = np.cumsum(count_mean) * np.diff(bins_count_mean)\n",
    "cdf_center = np.cumsum(count_center) * np.diff(bins_count_center)\n",
    "cdf_predictions = np.cumsum(count_predictions) * np.diff(bins_count_predictions)\n",
    "\n",
    "\n",
    "# plotting PDF and CDF\n",
    "plt.plot(bins_count_mean[1:], cdf_mean, label=\"Mean\")\n",
    "plt.plot(bins_count_center[1:], cdf_center, color=\"red\", label=\"Center\")\n",
    "plt.plot(bins_count_predictions[1:], cdf_predictions, color=\"magenta\", label=\"Predictions\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Prediction error (deg)\")\n",
    "plt.ylabel(\"Data Proportion\")\n",
    "#plt.title(\"CDF of Prediction and Baselines\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1731334502594,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "aPXff90UcQX7",
    "outputId": "aadb65aa-4649-408e-d423-1ea35ae3b6f3"
   },
   "outputs": [],
   "source": [
    "# Calculate means\n",
    "mean_model = np.mean(final_prediction_error)\n",
    "mean_mean = np.mean(final_mean_error)\n",
    "mean_center = np.mean(final_center_error)\n",
    "\n",
    "# Calculate standard deviations\n",
    "std_model = np.std(final_prediction_error, ddof=1)\n",
    "std_mean = np.std(final_mean_error, ddof=1)\n",
    "std_center = np.std(final_center_error, ddof=1)\n",
    "\n",
    "\n",
    "# Calculate SEMs\n",
    "sem_model = std_model / np.sqrt(len(final_prediction_error))\n",
    "sem_mean = std_model / np.sqrt(len(final_mean_error))\n",
    "sem_center = std_model / np.sqrt(len(final_center_error))\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "ci_model = [mean_model - 1.96 * sem_model, mean_model + 1.96 * sem_model]\n",
    "ci_mean = [mean_mean - 1.96 * sem_mean, mean_mean + 1.96 * sem_mean]\n",
    "ci_center = [mean_center - 1.96 * sem_center, mean_center + 1.96 * sem_center]\n",
    "\n",
    "\n",
    "\n",
    "# Check if confidence intervals overlap\n",
    "do_intervals_overlap = not (ci_model[1] < ci_mean[0] or ci_mean[1] < ci_model[0])\n",
    "\n",
    "# Print results\n",
    "print(f\"Model Mean Error:         {mean_model},               95% CI: {ci_model}\")\n",
    "print(f\"Mean Baseline Mean Error: {mean_mean},               95% CI: {ci_mean}\")\n",
    "if do_intervals_overlap:\n",
    "    print(\"The confidence intervals overlap, so the difference is NOT statistically significant.\")\n",
    "else:\n",
    "    print(\"The confidence intervals do NOT overlap, so the difference IS statistically significant.\")\n",
    "\n",
    "\n",
    "# Check if confidence intervals overlap\n",
    "do_intervals_overlap = not (ci_model[1] < ci_center[0] or ci_center[1] < ci_model[0])\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nModel Mean Error:           {mean_model},             95% CI: {ci_model}\")\n",
    "print(f\"Center Baseline Mean Error: {mean_center},            95% CI: {ci_center}\")\n",
    "if do_intervals_overlap:\n",
    "    print(\"The confidence intervals overlap, so the difference is NOT statistically significant.\")\n",
    "else:\n",
    "    print(\"The confidence intervals do NOT overlap, so the difference IS statistically significant.\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nModel SEM:  {sem_model} \\nMean SEM:   {sem_mean} \\nCenter SEM: {sem_center}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1732290599947,
     "user": {
      "displayName": "Konstantina Mammou",
      "userId": "05691654487090857473"
     },
     "user_tz": -120
    },
    "id": "vCgrrmXlcYTS",
    "outputId": "6fa85cd8-0165-497f-daac-41ec74ed369b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "# Create 2D histograms\n",
    "bins = 50\n",
    "range_limits = [[-50, 50], [-50, 50]]\n",
    "\n",
    "pred_hist, xedges, yedges = np.histogram2d(x1, y1, bins=bins, range=range_limits)\n",
    "gt_hist, _, _ = np.histogram2d(x2, y2, bins=bins, range=range_limits)\n",
    "\n",
    "# Normalize histograms to sum to 1 (for statistical comparison)\n",
    "pred_hist_norm = pred_hist / np.sum(pred_hist)\n",
    "gt_hist_norm = gt_hist / np.sum(gt_hist)\n",
    "\n",
    "# Calculate statistical metrics\n",
    "# 1. KL Divergence (flattened histograms)\n",
    "pred_flat = pred_hist_norm.flatten() + 1e-10  # Add small constant to avoid division by zero\n",
    "gt_flat = gt_hist_norm.flatten() + 1e-10\n",
    "kl_div = entropy(pred_flat, gt_flat)\n",
    "\n",
    "# 2. Mean Squared Error (MSE)\n",
    "mse = np.mean((pred_hist_norm - gt_hist_norm) ** 2)\n",
    "\n",
    "# Plot Predictions in the first figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist2d(x1, y1, bins=bins, range=range_limits, cmap=\"viridis\")\n",
    "plt.title(\"Predictions\")\n",
    "plt.xlabel(\"X (degrees)\")\n",
    "plt.ylabel(\"Y (degrees)\")\n",
    "plt.colorbar(label=\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Groundtruth in the second figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist2d(x2, y2, bins=bins, range=range_limits, cmap=\"viridis\")\n",
    "plt.title(\"Groundtruth\")\n",
    "plt.xlabel(\"X (degrees)\")\n",
    "plt.ylabel(\"Y (degrees)\")\n",
    "plt.colorbar(label=\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display metrics\n",
    "print(f\"KL Divergence: {kl_div:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOQ6+ZD60/oVpjDRw+QgWYv",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
